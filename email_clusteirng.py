# -*- coding: utf-8 -*-
"""Email Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OlrBUmEqKVa6vA-oGYd_79_bBmcVRDcq

# Primary Imports
"""

import pandas as pd # Data Prperocessing and Data Wrangling
import numpy as np # Data Wrangling and Data transfomations
import matplotlib.pyplot as plt # Visualization
import seaborn as sns # Visualization
import re #Regular Expression
import nltk # Natural Language Tool Kit
from nltk.corpus import stopwords # Stopwords
nltk.download('stopwords') # Stopwords
from nltk.stem import WordNetLemmatizer # Lemitaization
nltk.download('wordnet')
from sklearn.decomposition import PCA # Principle Component Analysis for Dimensionality Reduction

"""Data Import"""

df = pd.read_csv('https://raw.githubusercontent.com/anthdm/ml-email-clustering/master/split_emails.csv')

"""Eye Balling the dataset"""

df.head()

df.shape

df.info()

print(df['message'][0])

df['message'][0]

"""Data preprocessing"""

corpus = []
for i in df['message']:
  c = ''.join(i.split('X-FileName:')[-1].split('\n\n')[1:]).replace('\n', '')
  #if('To:' in c):
  corpus.append(c)
  #corpus.append(i.split('\n\n')[-2])
#  if(i.split('\n\n')[-1] == ''):
#    corpus.append(i.split('\n\n')[-2])
#  else:
#    corpus.append(i.split('\n\n')[-1])

df['corpus'] = corpus

df['corpus'].head()

"""Removing unnecessary white spaces and redundant emails from the dataset"""

c = set([])
for i in range(df['corpus'].shape[0]):
  df['corpus'][i] = re.sub(' +', ' ', df['corpus'][i])
  c.add(df['corpus'][i])
print(df['corpus'].shape)
print(len(c))

"""Out of 10000 emails only 5225 emails are unique, let's pick them up"""

a = [] 
removeable_index = []
for i in range(df['corpus'].shape[0]):
  x = df['corpus'][i]
  x = re.sub(' +', ' ', x)
  if(x not in a):
    a.append(x)
  else:
    removeable_index.append(i)

"""Removing Duplicate Emails"""

df.drop(removeable_index, axis = 0, inplace = True)

df.reset_index(drop=True,inplace = True)

"""Pre-processing the data for NLP"""

lem = WordNetLemmatizer()
clean_corpus = []

# This is an iterative step, I've run this notebook before and noticed that few not so important words are not filtered by the stopwords
# So i'm hard coding them into the machine
c = set(['com','sent','etc','ectcc','cc','bcc', 'message','subject','www','forward','th','hou','http','please','regards','thanks','ect','id','ee',
'eb','pm','ees','na'])
sw = set(stopwords.words('english')).union(c)

for i in df['corpus']:
  c = re.sub('[^A-Z,a-z]',' ',i).lower().strip().split()
  c = [lem.lemmatize(i) for i in c if i not in sw]
  clean_corpus.append(' '.join(c))

"""Pre-processed Corpus
---

Lemmatized each word using Wordnet Lementizer (Only the symantic root of a word is restored to reduce redundancy in the sparse matrix to be created in the further steps)

Removed the stop words using nltk.stopwords



"""

clean_corpus[0:5]

df['clean_corpus'] = clean_corpus

"""Final Dataset """

df.head()

"""Creating the BOW model using TfidfVectorizer"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf_idf = TfidfVectorizer(max_df = 0.5,min_df = 2)

vecs = tf_idf.fit_transform(clean_corpus).toarray()

print("Shape of the Bag of Words model created", vecs.shape)

"""The BOW model created is a pretty sparse matrix, with it-idf scores populated in it. For every word in every email(document)


---



The matrix's dimensions are as follows:

N-rows = No of emails in the dataset

N-cols = No of words identified by tf-idf vectorizer



---

"""

len(tf_idf.get_feature_names()) == vecs.shape[1]

"""Extracting the features with the highest TF-idf scores"""

top_fetaures = pd.DataFrame([tf_idf.get_feature_names(), vecs.sum(axis = 0).tolist()], index = ['Words','TF-iDF score']).T

top_fetaures = top_fetaures.sort_values(by = 'TF-iDF score', ascending = False).reset_index(drop = True)

first_50_words = top_fetaures.iloc[0:50,:]
second_50_words = top_fetaures.iloc[51:100,:]
third_50_words = top_fetaures.iloc[101:150,:]

f, axes = plt.subplots(1, 3, figsize=(30,15))
f.suptitle("Top words based on tf-idf score", fontsize=16)
axes[0].set_title("First 50 words", fontsize=14)
sns.barplot(y= first_50_words['Words'], x= first_50_words['TF-iDF score'], orient='h' , ax=axes[0], palette= 'Wistia_r')
axes[0].set_xlabel('tf-idf score', fontsize=12)
axes[0].set_ylabel('Words', fontsize=12)
axes[1].set_title("Second 50 words", fontsize=14)
sns.barplot(y= second_50_words['Words'], x= second_50_words['TF-iDF score'], orient='h' , ax=axes[1], palette= 'Wistia_r')
axes[1].set_xlabel('tf-idf score', fontsize=12)
axes[1].set_ylabel('Words', fontsize=12)
axes[2].set_title("Third 50 words", fontsize=14)
sns.barplot(x=third_50_words['TF-iDF score'], y= third_50_words['Words'], orient='h' , ax=axes[2],palette= 'Wistia_r')
axes[2].set_xlabel('tf-idf score', fontsize=12)
axes[2].set_ylabel('Words', fontsize=12)
plt.show()

"""# Checking the spread of the words in the emails, Using Principle Component Analysis.

"""

pca = PCA(n_components= 2)

model = pca.fit(vecs)
coords = pca.transform(vecs)

# Checking the spread of the data
plt.figure(figsize = (15,8))
plt.title("Checking the spread of Data Points using Principle Component Analysis")
plt.scatter(coords[:, 0], coords[:, 1], c='m')
plt.grid()
plt.show()

"""

---
# Clustering the BOW model to get email clusters. 

    Every row in the Bow model is a vector, 
    Thus what I've done here is i've projected the emails to vectors 
    and vectors are clustered. 

    Once we derive the clusters using K-means, 
    I will reproject the predictions on the emails again to derive email clusters. 
---




"""

from sklearn.cluster import KMeans

km = KMeans(n_clusters = 3, init = 'k-means++')

CLUSTERS = km.fit_predict(vecs) # DERIVING THE CLUSTERS

final_df =np.column_stack((vecs,CLUSTERS))

C = tf_idf.get_feature_names()
C.append('Clusters')
final_df = pd.DataFrame(final_df, columns = C)

final_df.drop(['ect','ce','cd','do','ng','via','web','ou','ees'] , axis = 1, inplace = True)

# Visualize the clusters here

"""

---
####Spliting the dataset cluster-wise
---




"""

df_c1 = final_df[final_df['Clusters']==0.0]
df_c2 = final_df[final_df['Clusters']==1.0]
df_c3 = final_df[final_df['Clusters']==2.0]

"""
---
#### Extracting the top 30 words in each cluster and visualizing them
---
"""

top_30_in_cluster_1 = df_c1.sum(axis = 0).sort_values(ascending = False).head(31)[1:]
top_30_in_cluster_2 = df_c2.sum(axis = 0).sort_values(ascending = False).head(31)[1:]
top_30_in_cluster_3 = df_c3.sum(axis = 0).sort_values(ascending = False).head(31)[1:]

f, axes = plt.subplots(1, 3, figsize=(30,15))
f.suptitle("Most used owrds under each Cluster", fontsize=16)
axes[0].set_title("top_30_words_in_cluster_1", fontsize=14)
sns.barplot(y=top_30_in_cluster_1.index, x= top_30_in_cluster_1, orient='h' , ax=axes[0], palette= 'Wistia_r')
axes[0].set_xlabel('tf-idf score', fontsize=12)
axes[0].set_ylabel('Words', fontsize=12)
axes[1].set_title("top_30_words_in_cluster_2", fontsize=14)
sns.barplot(y=top_30_in_cluster_2.index, x= top_30_in_cluster_2, orient='h' , ax=axes[1], palette= 'Wistia_r')
axes[1].set_xlabel('tf-idf score', fontsize=12)
axes[1].set_ylabel('Words', fontsize=12)
axes[2].set_title("top_30_words_in_cluster_3", fontsize=14)
sns.barplot(y=top_30_in_cluster_3.index, x= top_30_in_cluster_3, orient='h' , ax=axes[2],palette= 'Wistia_r')
axes[2].set_xlabel('tf-idf score', fontsize=12)
axes[2].set_ylabel('Words', fontsize=12)
plt.show()

# Inferences 
# Cluster one - We can see the names of companies, and words like stock, trademark, news etc. Form these words I claim these emails are 
# where discuss about market and generic market related emails

#Cluster two - We can see a nemaes like enron, email, day, call, information etc. From these words I claim 
# these emails are offical emails internal to enron

# Cluster three -  this could be generic emails like birthday wish/ level request/approvals
# from HR team.
# Not very business oriented emails

"""##### Implementing Cosine Similarity for find relationships between a highly used word Vs Emails

    From prior work on this notebook i know, 
    everything that exits the tf_ifd vectorizer is a vector (So is the nomenclature :D),

    So i've used the tf_idf object trained on the email corpus, 
    to transform a word from the top words under a cluster.

    The result is again a vector.

    By finding the cosine-simillarity score between a 
    "selected word's vectors Vs a email in the vector format" 
    gives the relationship between the word and the email.

    Using this logic I'm gathereing the top 10 related emails for a 
    top used word and prompting it.




"""

# The corpus need to be hard cleaned again to get rid of words like www, com, subject etc which are genric email terms.

# Let's see what cosine similarity logic has to say for the word "paribas" 

def Gimmi_to_n_similary_mails(x,n):
  v_query = tf_idf.transform([x])

  from sklearn.metrics.pairwise import linear_kernel
  cosine_sim = linear_kernel(v_query, vecs).flatten()
  
  related_email_indices = cosine_sim.argsort()[:-n:-1]

  for i in related_email_indices:
    print(df['corpus'][i], '\n\n', final_df['cluster'][i])
    print("==============================================================================================")

# Prints top 10 similary vectors of emails to the vector market and the cluster to which this mails belongs to
Gimmi_to_n_similary_mails('market',10)

# Prints top 15 similary vectors of emails to the vector approval and the cluster to which this mails belongs to
Gimmi_to_n_similary_mails('approval', 15)
