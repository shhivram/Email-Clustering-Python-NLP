# -*- coding: utf-8 -*-
"""Email Clusteirng.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OlrBUmEqKVa6vA-oGYd_79_bBmcVRDcq

# Primary Imports
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
from nltk.corpus import stopwords
nltk.download('punkt')
nltk.download('stopwords')
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
from sklearn.decomposition import PCA

"""Data Import"""

df = pd.read_csv('https://raw.githubusercontent.com/anthdm/ml-email-clustering/master/split_emails.csv')

"""Eye Balling the dataset"""

df.head()

df.shape

df.info()

print(df['message'][0])

df['message'][0]

"""Data preprocessing"""

corpus = []
for i in df['message']:
  c = ''.join(i.split('X-FileName:')[-1].split('\n\n')[1:]).replace('\n', '')
  #if('To:' in c):
  corpus.append(c)
  #corpus.append(i.split('\n\n')[-2])
#  if(i.split('\n\n')[-1] == ''):
#    corpus.append(i.split('\n\n')[-2])
#  else:
#    corpus.append(i.split('\n\n')[-1])

"""Dropping the forwarded messages"""

drop_inde = []
for i in range(len(corpus)):
  if(corpus[i].startswith('--------')):
    drop_inde.append(i)
  else:
    pass

len(drop_inde)

df['corpus'] = corpus
df.drop(drop_inde, axis =0, inplace = True)
df.drop('Unnamed: 0', axis =1, inplace = True)
df.reset_index(drop = True, inplace = True)

"""Deriving the body of each email using string formatting in python"""

df['corpus']

"""Pre-processing the data for NLP"""

lem = WordNetLemmatizer()
clean_corpus = []
for i in df['corpus']:
  c = re.sub('[^A-Z,a-z]',' ',i).lower().split()
  c = [lem.lemmatize(i) for i in c if i not in set(stopwords.words('english'))]
  clean_corpus.append(' '.join(c))

"""Pre-processed Corpus
---

Lemmatized each word using Wordnet Lementizer (Only the symantic root of a word is restored to reduce redundancy in the sparse matrix to be created in the further steps)

Removed the stop words using nltk.stopwords



"""

clean_corpus[0:5]

df['clean_corpus'] = clean_corpus

"""Final Dataset """

df.head()

"""Creating the BOW model using TfidfVectorizer"""

from sklearn.feature_extraction.text import TfidfVectorizer

tf_idf = TfidfVectorizer(max_df = 0.5,min_df = 2)

vecs = tf_idf.fit_transform(clean_corpus).toarray()

print("Shape of the Bag of Words model created", vecs.shape)

"""The BOW model created is a pretty sparse matrix, with it-idf scores populated in it. For every word in every email(document)


---



The matrix's dimensions are as follows:

N-rows = No of emails in the dataset

N-cols = No of words identified by tf-idf vectorizer



---

"""

len(tf_idf.get_feature_names()) == vecs.shape[1]

"""Extracting the features with the highest TF-idf scores"""

top_fetaures = pd.DataFrame([tf_idf.get_feature_names(), vecs.sum(axis = 0).tolist()], index = ['Words','TF-iDF score']).T

top_fetaures = top_fetaures.sort_values(by = 'TF-iDF score', ascending = False).reset_index(drop = True)

first_50_words = top_fetaures.iloc[0:50,:]
second_50_words = top_fetaures.iloc[51:100,:]
third_50_words = top_fetaures.iloc[101:150,:]

f, axes = plt.subplots(1, 3, figsize=(30,15))
f.suptitle("Top words based on tf-idf score", fontsize=16)
axes[0].set_title("First 50 words", fontsize=14)
sns.barplot(y= first_50_words['Words'], x= first_50_words['TF-iDF score'], orient='h' , ax=axes[0], palette= 'Wistia_r')
axes[0].set_xlabel('tf-idf score', fontsize=12)
axes[0].set_ylabel('Words', fontsize=12)
axes[1].set_title("Second 50 words", fontsize=14)
sns.barplot(y= second_50_words['Words'], x= second_50_words['TF-iDF score'], orient='h' , ax=axes[1], palette= 'Wistia_r')
axes[1].set_xlabel('tf-idf score', fontsize=12)
axes[1].set_ylabel('Words', fontsize=12)
axes[2].set_title("Third 50 words", fontsize=14)
sns.barplot(x=third_50_words['TF-iDF score'], y= third_50_words['Words'], orient='h' , ax=axes[2],palette= 'Wistia_r')
axes[2].set_xlabel('tf-idf score', fontsize=12)
axes[2].set_ylabel('Words', fontsize=12)
plt.show()

"""# Checking the spread of the words in the emails, Using Principle Component Analysis.

"""

pca = PCA(n_components= 200)

model = pca.fit(vecs)
coords = pca.transform(vecs)

# Checking the spread of the data
plt.figure(figsize = (15,8))
plt.title("Checking the spread of Data Points using Principle Component Analysis")
plt.scatter(coords[:, 0], coords[:, 1], c='m')
plt.grid()
plt.show()

"""

---
# Clustering the BOW model to get email clusters. 

    Every row in the Bow model is a vector, 
    Thus what I've done here is i've projected the emails to vectors 
    and vectors are clustered. 

    Once we derive the clusters using K-means, 
    I will reproject the predictions on the emails again to derive email clusters. 
---




"""

from sklearn.cluster import KMeans

km = KMeans(n_clusters = 3, init = 'k-means++')

CLUSTERS = km.fit_predict(vecs) # DERIVING THE CLUSTERS

final_df =np.column_stack((vecs,CLUSTERS))

C = tf_idf.get_feature_names()
C.append('Clusters')
final_df = pd.DataFrame(final_df, columns = C)

final_df.head()

# Visualize the clusters here

"""

---
####Spliting the dataset cluster-wise
---




"""

df_c1 = final_df[final_df['Clusters']==0.0]
df_c2 = final_df[final_df['Clusters']==1.0]
df_c3 = final_df[final_df['Clusters']==2.0]

"""
---
#### Extracting the top 30 words in each cluster and visualizing them
---
"""

top_30_in_cluster_1 = df_c1.sum(axis = 0).sort_values(ascending = False).head(31)[1:]
top_30_in_cluster_2 = df_c2.sum(axis = 0).sort_values(ascending = False).head(31)[1:]
top_30_in_cluster_3 = df_c3.sum(axis = 0).sort_values(ascending = False).head(31)[1:]

f, axes = plt.subplots(1, 3, figsize=(30,15))
f.suptitle("Most used owrds under each Cluster", fontsize=16)
axes[0].set_title("top_30_words_in_cluster_1", fontsize=14)
sns.barplot(y=top_30_in_cluster_1.index, x= top_30_in_cluster_1, orient='h' , ax=axes[0], palette= 'Wistia_r')
axes[0].set_xlabel('tf-idf score', fontsize=12)
axes[0].set_ylabel('Words', fontsize=12)
axes[1].set_title("top_30_words_in_cluster_2", fontsize=14)
sns.barplot(y=top_30_in_cluster_2.index, x= top_30_in_cluster_2, orient='h' , ax=axes[1], palette= 'Wistia_r')
axes[1].set_xlabel('tf-idf score', fontsize=12)
axes[1].set_ylabel('Words', fontsize=12)
axes[2].set_title("top_30_words_in_cluster_3", fontsize=14)
sns.barplot(y=top_30_in_cluster_3.index, x= top_30_in_cluster_3, orient='h' , ax=axes[2],palette= 'Wistia_r')
axes[2].set_xlabel('tf-idf score', fontsize=12)
axes[2].set_ylabel('Words', fontsize=12)
plt.show()

"""##### Implementing Cosine Similarity for find relationships between a highly used word Vs Emails

    From prior work on this notebook i know, 
    everything that exits the tf_ifd vectorizer is a vector (So is the nomenclature :D),

    So i've used the tf_idf object trained on the email corpus, 
    to transform a word from the top words under a cluster.

    The result is again a vector.

    By finding the cosine-simillarity score between a 
    "selected word's vectors Vs a email in the vector format" 
    gives the relationship between the word and the email.

    Using this logic I'm gathereing the top 10 related emails for a 
    top used word and prompting it.




"""

# The corpus need to be hard cleaned again to get rid of words like www, com, subject etc which are genric email terms.

# Let's see what cosine similarity logic has to say for the word "paribas" 

v_query = tf_idf.transform(['paribas'])

from sklearn.metrics.pairwise import linear_kernel
cosine_sim = linear_kernel(v_query, vecs).flatten()

related_email_indices = cosine_sim.argsort()[:-10:-1]
print(related_email_indices)

for i in related_email_indices:
  print(df['corpus'][i])

